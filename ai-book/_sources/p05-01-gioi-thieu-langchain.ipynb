{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8501dda-26ef-4fe8-8802-1819f38f2397",
   "metadata": {},
   "source": [
    "# Giới thiệu langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96170fe2-c74b-446e-ac59-3d9780f92028",
   "metadata": {},
   "source": [
    "Với sự phát triển mạnh mẽ của GenAI, các mô hình ngôn ngữ lớn LLM được sử dụng ngày càng rộng rãi với nhiều ứng dụng khác nhau. Tuy nhiên, các mô hình LLM này có các nhược điểm sau:\n",
    "\n",
    "- Kiến thức lỗi thời\n",
    "- Không thể thực hiện hành động, tương tác như tính toán, tra cứu\n",
    "- Thiếu bối cảnh - gặp khó khăn trong việc kết hợp bối cảnh liên quan trước và sau đó để có các phản hồi hữu ích\n",
    "- Ảo giác (`hallucination`) - LLM tự tạo ra các nội dung vô nghĩa hoặc không chính xác\n",
    "- Thiếu minh bạch & thiên lệch, phụ thuộc vào dữ liệu được huấn luyện"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcea705-7898-482d-a205-70709b6a0385",
   "metadata": {},
   "source": [
    "Để giải quyết và hạn chế các nhược điểm trên, có các kỹ thuật sau:\n",
    "\n",
    "- Tăng cường truy xuất (`retrieval augmentation`) - bổ sung kiến thức cho dữ liệu đào tạo đã lỗi thời của LLM, cung cấp các bối cảnh bên ngoài và giảm nguy cơ ảo giác\n",
    "- Chuỗi (`chaining`) - tích hợp thêm các hành động bên ngoài\n",
    "- Nhắc nhở (`prompt engineer`) - đưa ra các bối cảnh quan trọng để hướng dẫn các phản hồi\n",
    "- Giám sát, lọc và đánh giá - đưa thêm đánh giá của con người trong luồng phản hồi của máy\n",
    "- Tăng cường bộ nhớ (`memory`)\n",
    "- Tinh chỉnh mô hình (`fine tuning`) - huấn luyện lại mô hình LLM cho 1 số tác vụ cụ thể"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f681b2df-2bb1-42cb-8495-6e6c6bea423d",
   "metadata": {},
   "source": [
    "`langchain` là một framework trên python, được ra đời vào năm 2022 để xây dựng các ứng dụng hỗ trợ LLM thông qua việc xây dựng các module có sẵn. `langchain` có các cấu phần sau:\n",
    "\n",
    "- Chuỗi (`chain`): Chain là một tập hợp các bước hoặc mô-đun liên tiếp, nơi đầu ra của một bước có thể trở thành đầu vào cho bước tiếp theo, giúp tự động hóa quy trình và quản lý các tác vụ phức tạp mà yêu cầu nhiều bước xử lý.\n",
    "- Bộ công cụ (`tools`) - là các chức năng, mỗi tool thực hiện 1 tác vụ cụ thể. Ví dụ - tính toán, tra cứu hoặc kiểm tra thời tiết.\n",
    "- Agent - là bộ điều phối, xác định khi nào và cách thức sử dụng tool. Agent sử dụng LLM để hiểu như cầu từ người dùng, quyết định tool phù hợp, gọi các tool theo thứ tự thích hơn và tổng hợp kết quả.\n",
    "- Bộ nhớ (`memory`) - là trạng thái tồn tại giữa các lần thực hiện chuỗi hoặc agent. Các bộ nhớ có thể lưu trữ ngắn hạn hoặc dài hạn hoặc toàn bộ lịch sử, phụ thuộc vào chi phí thực hiện và mục tiêu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56fe0c-6ceb-4148-8041-ffe04ca5cdb9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Khi nào dùng Agent hoặc Tool?**\n",
    "\n",
    "| **Kịch bản**                                   | **Sử dụng Agent**                                                | **Sử dụng Tools**                                           |\n",
    "|------------------------------------------------|-------------------------------------------------------------------|------------------------------------------------------------|\n",
    "| **Cần quyết định khi nào và cách sử dụng công cụ** | ✅ Agent sẽ chọn tool dựa trên ngữ cảnh và yêu cầu.               | ❌ Tools không tự quyết định mà phải được gọi thủ công.     |\n",
    "| **Nhiệm vụ cụ thể**                            | ❌ Không cần agent, chỉ cần gọi tool phù hợp.                      | ✅ Tools xử lý nhanh gọn một nhiệm vụ độc lập.              |\n",
    "| **Xử lý tác vụ phức tạp, nhiều bước**          | ✅ Agent điều phối nhiều tool để giải quyết nhiệm vụ phức tạp.     | ❌ Tools không đủ khả năng xử lý nhiệm vụ nhiều bước.       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef95b932-34e4-4a4e-acd3-c4dad8aa8669",
   "metadata": {},
   "source": [
    "Trong chương này sẽ giới thiệu nhanh các khái niệm & ứng dụng cơ bản của `langchain`:\n",
    "\n",
    "- Document - Object lưu trữ dữ liệu text & metadata\n",
    "- Wrapper LLM - Mô hình LLM\n",
    "- Embedding - Chuyển đổi text sang vector embedding\n",
    "- Vector Store - Cơ sở dữ liệu cho phép lưu trữ vector embedding\n",
    "- Retriver - Truy xuất dữ liệu từ vector stor\n",
    "- Chat model - Mô hình chatbot sử dụng LLM\n",
    "- Prompt template - Xây dựng template cho chatbot\n",
    "- Memory - Cơ chế và cách thức lưu trữ lịch sử tương tác\n",
    "- Chain - Cách thức tạo ra một chuỗi liên kết với nhau để thực hiện một nhiệm vụ\n",
    "- Agent - Robot hay còn gọi là tác nhân, cho phép phân tích và thực hiện các tác vụ, bao gồm cả trong và ngoài LLM\n",
    "\n",
    "---\n",
    "\n",
    "Các thư viện python sẽ sử dụng\n",
    "\n",
    "- `langchain` & hệ sinh thái langchain: Thực hiện các ứng dụng GenAI\n",
    "- `pinecone-client`: Thực hiện vector database với pinecone (cần API)\n",
    "- `openai`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162de9d7-a856-4afa-8a12-f8c5ebecb020",
   "metadata": {},
   "source": [
    "## Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4081a99-d5e7-4e5e-8285-7b6e391356e7",
   "metadata": {},
   "source": [
    "`Document` là một đối tượng có chứa text và các metadata veef tài liệu đó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3f49f78-4764-4dfc-8f4b-39970e0bcee4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:55:12.990784Z",
     "iopub.status.busy": "2025-01-14T08:55:12.990455Z",
     "iopub.status.idle": "2025-01-14T08:55:12.994515Z",
     "shell.execute_reply": "2025-01-14T08:55:12.993710Z",
     "shell.execute_reply.started": "2025-01-14T08:55:12.990762Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "888e2e3c-8cf3-4d64-825d-8f0f1e12527d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:55:13.000318Z",
     "iopub.status.busy": "2025-01-14T08:55:13.000062Z",
     "iopub.status.idle": "2025-01-14T08:55:13.006629Z",
     "shell.execute_reply": "2025-01-14T08:55:13.005441Z",
     "shell.execute_reply.started": "2025-01-14T08:55:13.000297Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'my_document_id': 234234, 'my_document_source': 'The LangChain Papers', 'my_document_create_time': 1680013019}, page_content=\"This is my document. It is full of text that I've gathered from other places\")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\",\n",
    "         metadata={\n",
    "             'my_document_id' : 234234,\n",
    "             'my_document_source' : \"The LangChain Papers\",\n",
    "             'my_document_create_time' : 1680013019\n",
    "         })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58715c4-305d-40cd-af09-0a14e482f4d0",
   "metadata": {},
   "source": [
    "Các thông tin về document trong `langchain` là không bắt buộc, ta có thể tạo ra các document mà không cần metadata như sau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d5c18e3f-a921-4692-aa06-90cf3904c58c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:55:13.009388Z",
     "iopub.status.busy": "2025-01-14T08:55:13.008953Z",
     "iopub.status.idle": "2025-01-14T08:55:13.014470Z",
     "shell.execute_reply": "2025-01-14T08:55:13.013694Z",
     "shell.execute_reply.started": "2025-01-14T08:55:13.009357Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='Thông tin & hiểu biết về langchain là các kiến thức hữu ích')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content = \"Thông tin & hiểu biết về langchain là các kiến thức hữu ích\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986721dd-9384-4143-b1a7-881d6d0c3131",
   "metadata": {},
   "source": [
    "## Wrappers LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbddf94-e0d4-429d-bf0d-86fc10453b30",
   "metadata": {},
   "source": [
    "`langchain` cho phép sử dụng nhiều mô hình LLM khác nhau, bao gồm cả open-source & enterprise. Để đơn giản, ta sẽ sử dụng mô hình LLM `enterprise` với OpenAI. Key của LLM được lưu trữ trong `environment` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e69d71a-056b-43f8-863b-1903dfb61fbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:55:13.015846Z",
     "iopub.status.busy": "2025-01-14T08:55:13.015606Z",
     "iopub.status.idle": "2025-01-14T08:55:13.021619Z",
     "shell.execute_reply": "2025-01-14T08:55:13.020983Z",
     "shell.execute_reply.started": "2025-01-14T08:55:13.015827Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d624b8c-f841-4bfe-97d6-1ce232ef81c8",
   "metadata": {},
   "source": [
    "Ta có thể khởi tạo một mô hình LLM cơ bản như sau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0751291-4e88-48d5-b476-b2cf833b3b82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T21:33:01.665701Z",
     "iopub.status.busy": "2025-01-15T21:33:01.665380Z",
     "iopub.status.idle": "2025-01-15T21:33:03.228861Z",
     "shell.execute_reply": "2025-01-15T21:33:03.227515Z",
     "shell.execute_reply.started": "2025-01-15T21:33:01.665675Z"
    }
   },
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m llm(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplain large language models in one sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.11/site-packages/langchain_core/load/serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:613\u001b[0m, in \u001b[0;36mBaseChatOpenAI.validate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy)\n\u001b[1;32m    612\u001b[0m     sync_specific \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client}\n\u001b[0;32m--> 613\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msync_specific\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.11/site-packages/openai/_client.py:110\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    108\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "llm(\"explain large language models in one sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7da1b40-67e0-4f06-bc26-f8a046be1dda",
   "metadata": {},
   "source": [
    "## Chat model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070480de-9abc-40a2-a356-e9c76649980f",
   "metadata": {},
   "source": [
    "Khi tương tác với LLM thông qua chatbot, có 3 nhóm thông tin\n",
    "\n",
    "- **System**: Thông tin background cho phép AI biết cần phải làm gì\n",
    "- **Human**: Thông tin của người dùng\n",
    "- **AI**: Thông tin AI phản hồi người dùng "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e763932-2aca-44cb-a34e-9496bb0a010b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:55:13.971765Z",
     "iopub.status.busy": "2025-01-14T08:55:13.971422Z",
     "iopub.status.idle": "2025-01-14T08:55:13.997342Z",
     "shell.execute_reply": "2025-01-14T08:55:13.996722Z",
     "shell.execute_reply.started": "2025-01-14T08:55:13.971737Z"
    }
   },
   "outputs": [],
   "source": [
    "# import schema for chat messages and ChatOpenAI in order to query chatmodels GPT-3.5-turbo or GPT-4\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\",temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c37807a1-3e47-48c9-ad43-2b4e9a6abcbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:55:13.998751Z",
     "iopub.status.busy": "2025-01-14T08:55:13.998455Z",
     "iopub.status.idle": "2025-01-14T08:55:16.761829Z",
     "shell.execute_reply": "2025-01-14T08:55:16.761035Z",
     "shell.execute_reply.started": "2025-01-14T08:55:13.998722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here is an example Python script that trains a simple neural network on simulated data using the TensorFlow library:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "# Generate simulated data\n",
      "np.random.seed(0)\n",
      "X = np.random.rand(1000, 2)\n",
      "y = np.array([1 if x1 + x2 > 1 else 0 for x1, x2 in X])\n",
      "\n",
      "# Define the neural network architecture\n",
      "model = tf.keras.models.Sequential([\n",
      "    tf.keras.layers.Dense(64, activation='relu', input_shape=(2,)),\n",
      "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
      "])\n",
      "\n",
      "# Compile the model\n",
      "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
      "\n",
      "# Train the model\n",
      "model.fit(X, y, epochs=10, batch_size=32)\n",
      "\n",
      "# Evaluate the model\n",
      "loss, accuracy = model.evaluate(X, y)\n",
      "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
      "```\n",
      "\n",
      "In this script:\n",
      "1. We generate a simulated dataset with 1000 samples and 2 features.\n",
      "2. We define a simple neural network with one hidden layer of 64 neurons and an output layer with a sigmoid activation function.\n",
      "3. We compile the model using the Adam optimizer and binary cross-entropy loss.\n",
      "4. We train the model on the simulated data for 10 epochs with a batch size of 32.\n",
      "5. We evaluate the model on the training data and print the loss and accuracy.\n",
      "\n",
      "You can run this script in a Python environment with TensorFlow installed to train a neural network on simulated data.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert data scientist\"),\n",
    "    HumanMessage(content=\"Write a Python script that trains a neural network on simulated data \")\n",
    "]\n",
    "response=chat(messages)\n",
    "\n",
    "print(response.content,end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0dca76-2b9b-4ddd-8ba3-d97a0be96835",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Ta cũng có thể loại bỏ hoặc đưa đầy đủ cả hướng dẫn với `system message` như sau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ee33adf-2daa-4bf7-9481-d468731d1f91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:55:16.763216Z",
     "iopub.status.busy": "2025-01-14T08:55:16.762847Z",
     "iopub.status.idle": "2025-01-14T08:55:21.710668Z",
     "shell.execute_reply": "2025-01-14T08:55:21.709894Z",
     "shell.execute_reply.started": "2025-01-14T08:55:16.763190Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. Phú Quốc: Đảo ngọc Phú Quốc nằm ở phía Nam của Việt Nam, nổi tiếng với bãi biển đẹp và nước biển trong xanh. Du khách có thể tham gia các hoạt động như lặn biển, thăm các hòn đảo lân cận, thưởng thức hải sản tươi ngon.\\n\\n2. Nha Trang: Nha Trang là một trong những điểm du lịch biển phổ biến nhất ở Việt Nam, với bãi biển dài và nước biển trong xanh. Du khách có thể tham gia các hoạt động như lặn biển, thăm các đảo lân cận, thưởng thức ẩm thực địa phương.\\n\\n3. Đà Nẵng: Đà Nẵng có nhiều bãi biển đẹp như Bãi biển Mỹ Khê, Bãi biển Non Nước, Bãi biển Nam Ô. Du khách cũng có thể tham quan các điểm du lịch nổi tiếng như Ngũ Hành Sơn, Bà Nà Hills, Hội An.\\n\\n4. Mũi Né: Mũi Né nổi tiếng với cát trắng và gió biển mạnh, là điểm đến lý tưởng cho các hoạt động như lướt ván buồm, lướt sóng, thăm các địa điểm tham quan như Đồi Cát Trắng, Suối Tiên, Đồi Cát Bay.\\n\\n5. Cửa Lò: Cửa Lò là một bãi biển yên bình ở Nghệ An, nổi tiếng với cát trắng và nước biển trong xanh. Du khách có thể tham gia các hoạt động như tắm biển, thăm các địa điểm lân cận như đền Cửa Lò, đảo Cồn Cỏ.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 543, 'prompt_tokens': 25, 'total_tokens': 568, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f9aa8b2c-e525-4c9c-8e67-64d279c847b0-0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        HumanMessage(content = \"Đi biển ở Việt Nam thì nên đi đâu\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "132d5d2d-546c-459c-abed-ed9d260a88f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:55:21.712417Z",
     "iopub.status.busy": "2025-01-14T08:55:21.711792Z",
     "iopub.status.idle": "2025-01-14T08:55:22.555369Z",
     "shell.execute_reply": "2025-01-14T08:55:22.554589Z",
     "shell.execute_reply.started": "2025-01-14T08:55:21.712383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Nếu bạn muốn tận hưởng biển xanh, cát trắng và không khí trong lành, hãy đến với Phú Quốc, Nha Trang, Đà Nẵng hoặc Hội An.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 72, 'total_tokens': 135, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c76de61c-66d5-468b-b9a6-027aa00ddab8-0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Có hướng dẫn\n",
    "chat(\n",
    "    [\n",
    "        SystemMessage(content = \"Bạn là trợ lý hướng dẫn viên du lịch và đưa gợi ý chỉ bằng một trả lời ngắn gọn\"),\n",
    "        HumanMessage(content = \"Đi biển ở Việt Nam thì nên đi đâu\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba4c84-d6e2-496a-abd6-e8303f68970d",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8626877d-db0b-4315-bf1d-7781a07d2123",
   "metadata": {},
   "source": [
    "`memory` là nhóm kỹ thuật cho phép LLM ghi nhớ về các thông tin trong quá trình tương tác"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e50050da-76c7-4e03-9893-fb4142464823",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:55:22.556591Z",
     "iopub.status.busy": "2025-01-14T08:55:22.556123Z",
     "iopub.status.idle": "2025-01-14T08:55:22.580662Z",
     "shell.execute_reply": "2025-01-14T08:55:22.580038Z",
     "shell.execute_reply.started": "2025-01-14T08:55:22.556558Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\",temperature=0.3)\n",
    "history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e1df1438-4942-4f72-bbee-24e7139b6962",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:55:22.583673Z",
     "iopub.status.busy": "2025-01-14T08:55:22.583416Z",
     "iopub.status.idle": "2025-01-14T08:55:43.475354Z",
     "shell.execute_reply": "2025-01-14T08:55:43.474601Z",
     "shell.execute_reply.started": "2025-01-14T08:55:22.583654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Thủ đô của Pháp là Paris.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 19, 'total_tokens': 31, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-95dfae4c-1efd-49d3-b945-3b3637838f62-0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        HumanMessage(content = \"Thủ đô của Pháp là gì?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b6a0d089-f92b-4448-9201-77f1caeaed3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:55:43.476541Z",
     "iopub.status.busy": "2025-01-14T08:55:43.476129Z",
     "iopub.status.idle": "2025-01-14T08:55:43.481409Z",
     "shell.execute_reply": "2025-01-14T08:55:43.480637Z",
     "shell.execute_reply.started": "2025-01-14T08:55:43.476510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kiểm tra memory\n",
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb30de29-9c58-49d4-a8bb-d3c0d1a4432e",
   "metadata": {},
   "source": [
    "Lúc này, object `memory` chưa được lưu trữ thông tin về cuộc hội thoại của người dùng, ta có thể lưu trữ lại như sau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "86551a3c-b47b-41cf-beef-931b6a45add6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:55:43.482910Z",
     "iopub.status.busy": "2025-01-14T08:55:43.482509Z",
     "iopub.status.idle": "2025-01-14T08:55:43.486919Z",
     "shell.execute_reply": "2025-01-14T08:55:43.486362Z",
     "shell.execute_reply.started": "2025-01-14T08:55:43.482882Z"
    }
   },
   "outputs": [],
   "source": [
    "history.add_ai_message(\"Xin chào! Tôi có thể giúp gì cho bạn?\")\n",
    "history.add_user_message(\"Thủ đô của Pháp là gì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1a64956b-32ee-4019-b60c-594603358346",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:55:43.488569Z",
     "iopub.status.busy": "2025-01-14T08:55:43.487525Z",
     "iopub.status.idle": "2025-01-14T08:55:43.494180Z",
     "shell.execute_reply": "2025-01-14T08:55:43.493586Z",
     "shell.execute_reply.started": "2025-01-14T08:55:43.488524Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Xin chào! Tôi có thể giúp gì cho bạn?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Thủ đô của Pháp là gì', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adb587a-ef30-4f93-b5be-791a3da7b232",
   "metadata": {},
   "source": [
    "## Prompts & Prompts template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97624e5a-c50f-466d-92f1-bdb5c4f1d0c3",
   "metadata": {},
   "source": [
    "`Prompt` là một đoạn văn bản hoặc yêu cầu mà người dùng nhập vào để hướng dẫn mô hình tạo ra kết quả. Prompt có thể là một câu hỏi, một chỉ dẫn cụ thể, hay một mô tả về hình ảnh, bài viết hoặc ý tưởng mà ta muốn mô hình phát triển hoặc phản hồi. Prompt giúp định hình nội dung và hướng đi cho mô hình AI để tạo ra kết quả phù hợp với yêu cầu của người sử dụng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "03664581-0f2e-417d-b36e-7e91f83c2a97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:55:43.495904Z",
     "iopub.status.busy": "2025-01-14T08:55:43.495651Z",
     "iopub.status.idle": "2025-01-14T08:55:44.342805Z",
     "shell.execute_reply": "2025-01-14T08:55:44.341931Z",
     "shell.execute_reply.started": "2025-01-14T08:55:43.495887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The statement is incorrect because if today is Monday, then tomorrow would be Tuesday, not Wednesday.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 23, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-2e4f38dd-401e-47a5-8793-6ceba41c405f-0' usage_metadata={'input_tokens': 23, 'output_tokens': 20, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "Today is Monday, tomorrow is Wednesday.\n",
    "What is wrong with that statement?\n",
    "\"\"\"\n",
    "\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed4f682-07c5-46fc-b122-3c7bbbaba279",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`Prompt template` là một khuôn mẫu được thiết kế sẵn để giúp người dùng tạo ra các prompts một cách dễ dàng và hiệu quả. Nó cung cấp một cấu trúc cụ thể mà người dùng có thể điền vào để tạo ra yêu cầu cho mô hình AI. Prompt template thường được sử dụng để đảm bảo rằng đầu vào được cung cấp có độ rõ ràng và đầy đủ, giúp mô hình AI tạo ra kết quả chính xác và phù hợp.\n",
    "\n",
    "Các prompt templates có thể bao gồm các thành phần cố định (ví dụ: cấu trúc câu) và các phần trống mà người dùng có thể điền vào để tùy chỉnh yêu cầu. Việc sử dụng template giúp tiết kiệm thời gian, giảm thiểu sự mơ hồ, và hướng đến mục tiêu rõ ràng hơn khi tương tác với AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a32ff4e9-e28f-41ef-be10-d8f2c0dac3d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:55:44.344761Z",
     "iopub.status.busy": "2025-01-14T08:55:44.344078Z",
     "iopub.status.idle": "2025-01-14T08:55:44.348470Z",
     "shell.execute_reply": "2025-01-14T08:55:44.347880Z",
     "shell.execute_reply.started": "2025-01-14T08:55:44.344731Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import prompt and define PromptTemplate\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert data scientist with an expertise in building deep learning models. \n",
    "Explain the concept of {concept} in a couple of lines\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"concept\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1325413a-3b26-47e7-a53c-8445d5e882ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:55:44.349830Z",
     "iopub.status.busy": "2025-01-14T08:55:44.349490Z",
     "iopub.status.idle": "2025-01-14T08:55:44.355165Z",
     "shell.execute_reply": "2025-01-14T08:55:44.354587Z",
     "shell.execute_reply.started": "2025-01-14T08:55:44.349802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an expert data scientist with an expertise in building deep learning models. \n",
      "Explain the concept of autoencoder in a couple of lines\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run LLM with PromptTemplate\n",
    "my_prompt = prompt.format(concept=\"autoencoder\")\n",
    "print(my_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8b5a2ee0-4dc9-426c-865d-948412b46404",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:55:44.356709Z",
     "iopub.status.busy": "2025-01-14T08:55:44.356015Z",
     "iopub.status.idle": "2025-01-14T08:55:45.590035Z",
     "shell.execute_reply": "2025-01-14T08:55:45.589279Z",
     "shell.execute_reply.started": "2025-01-14T08:55:44.356680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='An autoencoder is a type of neural network used for unsupervised learning, designed to encode input data into a lower-dimensional representation and then decode that representation back to the original input. It consists of two main components: an encoder, which compresses the data, and a decoder, which reconstructs the data, enabling tasks such as dimensionality reduction, anomaly detection, and data denoising.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 36, 'total_tokens': 117, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-4b825bd8-bc84-430e-aac8-f6e78cfe3f55-0', usage_metadata={'input_tokens': 36, 'output_tokens': 81, 'total_tokens': 117, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt.format(concept=\"autoencoder\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ed405-957f-4a29-b078-e2586963c479",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07853152-2ee3-410f-9390-35d4a10a6130",
   "metadata": {},
   "source": [
    "`Chain` là một chuỗi các bước được liên kết với nhau để thực hiện một tác vụ cụ thể với đầu ra của bước này là đầu vào của một bước khác. Chain cho phép ta tạo ra các quy trình phức tạp & dễ quản lý khi làm việc với mô hình ngôn ngữ lớn\n",
    "\n",
    "Ta có thể tạo chain với 2 bước như sau:\n",
    "\n",
    "- Bước 1: Giải thích 1 thuật ngữ kỹ thuật\n",
    "- Bước 2: Dùng kết quả đầu ra của bước 1, viết cô đọng và giải thích lại bằng ngôn ngữ đơn giản"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5884fc3d-aa08-4b07-976f-fed2ef75e7ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:55:45.591204Z",
     "iopub.status.busy": "2025-01-14T08:55:45.590818Z",
     "iopub.status.idle": "2025-01-14T08:55:46.862471Z",
     "shell.execute_reply": "2025-01-14T08:55:46.861612Z",
     "shell.execute_reply.started": "2025-01-14T08:55:45.591172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'concept': 'autoencoder', 'text': 'An autoencoder is a type of neural network designed to learn efficient representations of data, typically for the purpose of dimensionality reduction or feature learning. It consists of two main parts: an encoder that compresses the input into a lower-dimensional latent space and a decoder that reconstructs the original input from this representation. The model is trained to minimize the reconstruction error between the input and the output.'}\n"
     ]
    }
   ],
   "source": [
    "# Tạo chain\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.invoke(\"autoencoder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d27aeba1-445a-4c72-b1bd-f9a6a28adc8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:55:46.864102Z",
     "iopub.status.busy": "2025-01-14T08:55:46.863406Z",
     "iopub.status.idle": "2025-01-14T08:55:46.868209Z",
     "shell.execute_reply": "2025-01-14T08:55:46.867586Z",
     "shell.execute_reply.started": "2025-01-14T08:55:46.864073Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a second prompt \n",
    "\n",
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"ml_concept\"],\n",
    "    template=\"Turn the concept description of {ml_concept} and explain it to me like I'm five in 500 words\",\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03fa75f-6f44-4d79-ab97-924726bb94cc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Sau khi thực hiện xong 2 `chain`, ta có thể ghép lại thành 1 `chain` duy nhất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "691db599-8780-4a6e-9118-97319ce7c353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:55:46.869662Z",
     "iopub.status.busy": "2025-01-14T08:55:46.869240Z",
     "iopub.status.idle": "2025-01-14T08:57:10.187025Z",
     "shell.execute_reply": "2025-01-14T08:57:10.186215Z",
     "shell.execute_reply.started": "2025-01-14T08:55:46.869634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mAn autoencoder is a type of neural network designed to learn efficient representations (encodings) of input data by compressing it into a lower-dimensional space and then reconstructing the original input from that representation. It consists of two main components: an encoder that compresses the data and a decoder that reconstructs it, with the goal of minimizing the reconstruction error. Autoencoders are commonly used for tasks like dimensionality reduction, anomaly detection, and image denoising.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mAlright! Imagine you have a really big box of toy blocks, and you love to play with them. But sometimes, it gets really hard to find the blocks you want because there are just so many! So, what if you had a magic machine that could help you organize your blocks in a simpler way?\n",
      "\n",
      "This magic machine is called an **autoencoder**! Think of it like a super-smart toy organizer.\n",
      "\n",
      "### The Two Parts of the Magic Machine\n",
      "\n",
      "The autoencoder has two special parts: the **encoder** and the **decoder**.\n",
      "\n",
      "1. **The Encoder**: This is like the part of the machine that helps you squish all your blocks. When you put your big box of toys into this machine, the encoder looks at all the blocks and figures out a way to squeeze them down into a smaller, easier-to-handle box. It tries to remember what all of your blocks look like, but it keeps only the important bits. So instead of having a big messy box, you now have a smaller, organized one!\n",
      "\n",
      "2. **The Decoder**: Now, here’s where the magic happens! When you want to play with your toys again, you can use the decoder. This part of the machine takes that smaller box of organized blocks, and magically transforms it back into the big box of toys that looks just like it did before! It tries really hard to make sure that every block is back in its original shape and color.\n",
      "\n",
      "### Why Do We Use the Autoencoder?\n",
      "\n",
      "So, why do we need this super-smart organizer? Well, there are a few fun reasons!\n",
      "\n",
      "- **Making Things Simpler**: Just like how it’s easier to play with a few organized blocks rather than a huge pile, the autoencoder helps people make complicated pictures or sounds easier to understand by turning them into simpler forms. This makes your computer work faster and smarter!\n",
      "\n",
      "- **Finding Hidden Treasure**: Sometimes, when playing with your toys, you find some blocks that don’t belong. Those are like surprises or mistakes, and the autoencoder can help you find them! With the smaller organized box, it can tell if a block is out of place. This is called **anomaly detection**, which is a fancy way of saying it helps you find things that don’t fit.\n",
      "\n",
      "- **Cleaning Up Messy Drawings**: Imagine if you drew a picture, but it was all smudged and messy. The autoencoder can help clean it up! It can learn how to fix those mistakes so your picture looks beautiful again. This is called **image denoising**.\n",
      "\n",
      "### The Goal of the Autoencoder\n",
      "\n",
      "The main job of the autoencoder is to make sure it does a very good job squeezing and then re-building the toy blocks. It always tries its best to make sure the blocks look the same after they come out of the machine. The better it gets, the less difference there is between the way the toys looked before and after!\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "So, just like your magic toy organizer that helps turn a big mess into a neat little box of blocks and back again, autoencoders are smart machines that help us understand and simplify data. They are like trusty helpers in a world full of information! Whether it’s to make sense of pictures, find odd things, or clean up the messes, autoencoders are there to lend a helping hand. How cool is that?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'autoencoder', 'output': 'Alright! Imagine you have a really big box of toy blocks, and you love to play with them. But sometimes, it gets really hard to find the blocks you want because there are just so many! So, what if you had a magic machine that could help you organize your blocks in a simpler way?\\n\\nThis magic machine is called an **autoencoder**! Think of it like a super-smart toy organizer.\\n\\n### The Two Parts of the Magic Machine\\n\\nThe autoencoder has two special parts: the **encoder** and the **decoder**.\\n\\n1. **The Encoder**: This is like the part of the machine that helps you squish all your blocks. When you put your big box of toys into this machine, the encoder looks at all the blocks and figures out a way to squeeze them down into a smaller, easier-to-handle box. It tries to remember what all of your blocks look like, but it keeps only the important bits. So instead of having a big messy box, you now have a smaller, organized one!\\n\\n2. **The Decoder**: Now, here’s where the magic happens! When you want to play with your toys again, you can use the decoder. This part of the machine takes that smaller box of organized blocks, and magically transforms it back into the big box of toys that looks just like it did before! It tries really hard to make sure that every block is back in its original shape and color.\\n\\n### Why Do We Use the Autoencoder?\\n\\nSo, why do we need this super-smart organizer? Well, there are a few fun reasons!\\n\\n- **Making Things Simpler**: Just like how it’s easier to play with a few organized blocks rather than a huge pile, the autoencoder helps people make complicated pictures or sounds easier to understand by turning them into simpler forms. This makes your computer work faster and smarter!\\n\\n- **Finding Hidden Treasure**: Sometimes, when playing with your toys, you find some blocks that don’t belong. Those are like surprises or mistakes, and the autoencoder can help you find them! With the smaller organized box, it can tell if a block is out of place. This is called **anomaly detection**, which is a fancy way of saying it helps you find things that don’t fit.\\n\\n- **Cleaning Up Messy Drawings**: Imagine if you drew a picture, but it was all smudged and messy. The autoencoder can help clean it up! It can learn how to fix those mistakes so your picture looks beautiful again. This is called **image denoising**.\\n\\n### The Goal of the Autoencoder\\n\\nThe main job of the autoencoder is to make sure it does a very good job squeezing and then re-building the toy blocks. It always tries its best to make sure the blocks look the same after they come out of the machine. The better it gets, the less difference there is between the way the toys looked before and after!\\n\\n### Conclusion\\n\\nSo, just like your magic toy organizer that helps turn a big mess into a neat little box of blocks and back again, autoencoders are smart machines that help us understand and simplify data. They are like trusty helpers in a world full of information! Whether it’s to make sense of pictures, find odd things, or clean up the messes, autoencoders are there to lend a helping hand. How cool is that?'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "# Thực hiện toàn bộ chain với input đầu vào từ chain 1\n",
    "explanation = overall_chain.invoke(\"autoencoder\")\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8f2281-b5c3-4631-89bc-68ef41de6596",
   "metadata": {},
   "source": [
    "## Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b336943-cdaf-47be-b3da-7cb21eefbada",
   "metadata": {},
   "source": [
    "Với 1 đoạn văn bản, ta cần chia nhỏ thành các đoạn được gọi là `chunk`. Với langchain, có cấu phần chia nhỏ text thành các chunk với 2 thuộc tính chính:\n",
    "\n",
    "- chunk_size: Số lượng ký tự tối đa cho 1 chunk\n",
    "- chunk_overlap: Cho phép overlap giữa 2 đoạn liên tiếp. `chunk_overlap = 0` nghĩa là không cho phép trùng lặp giữa 2 đoạn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "87f88df5-9b5a-4fe7-bfb5-c4ec5a819a22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:57:10.188457Z",
     "iopub.status.busy": "2025-01-14T08:57:10.188136Z",
     "iopub.status.idle": "2025-01-14T08:57:10.192590Z",
     "shell.execute_reply": "2025-01-14T08:57:10.191776Z",
     "shell.execute_reply.started": "2025-01-14T08:57:10.188430Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap  = 0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\", \".\", \"!\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f768286d-b0a0-43de-867f-190428bfafae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:57:10.194002Z",
     "iopub.status.busy": "2025-01-14T08:57:10.193557Z",
     "iopub.status.idle": "2025-01-14T08:57:10.199997Z",
     "shell.execute_reply": "2025-01-14T08:57:10.199349Z",
     "shell.execute_reply.started": "2025-01-14T08:57:10.193972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alright! Imagine you have a really big box of toy blocks, and you love to play with them. But sometimes, it gets really hard to find the blocks you want because there are just so many! So, what if you had a magic machine that could help you organize your blocks in a simpler way?\\n\\nThis magic machine is called an **autoencoder**! Think of it like a super-smart toy organizer.\\n\\n### The Two Parts of the Magic Machine\\n\\nThe autoencoder has two special parts: the **encoder** and the **decoder**.\\n\\n1. **The Encoder**: This is like the part of the machine that helps you squish all your blocks. When you put your big box of toys into this machine, the encoder looks at all the blocks and figures out a way to squeeze them down into a smaller, easier-to-handle box. It tries to remember what all of your blocks look like, but it keeps only the important bits. So instead of having a big messy box, you now have a smaller, organized one!\\n\\n2. **The Decoder**: Now, here’s where the magic happens! When you want to play with your toys again, you can use the decoder. This part of the machine takes that smaller box of organized blocks, and magically transforms it back into the big box of toys that looks just like it did before! It tries really hard to make sure that every block is back in its original shape and color.\\n\\n### Why Do We Use the Autoencoder?\\n\\nSo, why do we need this super-smart organizer? Well, there are a few fun reasons!\\n\\n- **Making Things Simpler**: Just like how it’s easier to play with a few organized blocks rather than a huge pile, the autoencoder helps people make complicated pictures or sounds easier to understand by turning them into simpler forms. This makes your computer work faster and smarter!\\n\\n- **Finding Hidden Treasure**: Sometimes, when playing with your toys, you find some blocks that don’t belong. Those are like surprises or mistakes, and the autoencoder can help you find them! With the smaller organized box, it can tell if a block is out of place. This is called **anomaly detection**, which is a fancy way of saying it helps you find things that don’t fit.\\n\\n- **Cleaning Up Messy Drawings**: Imagine if you drew a picture, but it was all smudged and messy. The autoencoder can help clean it up! It can learn how to fix those mistakes so your picture looks beautiful again. This is called **image denoising**.\\n\\n### The Goal of the Autoencoder\\n\\nThe main job of the autoencoder is to make sure it does a very good job squeezing and then re-building the toy blocks. It always tries its best to make sure the blocks look the same after they come out of the machine. The better it gets, the less difference there is between the way the toys looked before and after!\\n\\n### Conclusion\\n\\nSo, just like your magic toy organizer that helps turn a big mess into a neat little box of blocks and back again, autoencoders are smart machines that help us understand and simplify data. They are like trusty helpers in a world full of information! Whether it’s to make sense of pictures, find odd things, or clean up the messes, autoencoders are there to lend a helping hand. How cool is that?'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanation['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2c071c4c-6507-4436-96c3-a9eceb6508ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:57:10.202365Z",
     "iopub.status.busy": "2025-01-14T08:57:10.201136Z",
     "iopub.status.idle": "2025-01-14T08:57:10.206682Z",
     "shell.execute_reply": "2025-01-14T08:57:10.206052Z",
     "shell.execute_reply.started": "2025-01-14T08:57:10.202339Z"
    }
   },
   "outputs": [],
   "source": [
    "# Chia thành chunk\n",
    "texts = text_splitter.create_documents([explanation['output']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7497e8-3781-47b1-90e6-fe77d5a1a336",
   "metadata": {},
   "source": [
    "Các đoạn text riêng biệt có thể xem với `page_content`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "50988f57-79cf-4982-bb6b-bfd11756dea5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:57:10.208447Z",
     "iopub.status.busy": "2025-01-14T08:57:10.207832Z",
     "iopub.status.idle": "2025-01-14T08:57:10.212902Z",
     "shell.execute_reply": "2025-01-14T08:57:10.212323Z",
     "shell.execute_reply.started": "2025-01-14T08:57:10.208415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alright! Imagine you have a really big box of toy blocks, and you love to play with them. But'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcde7005-9bef-45d9-a68c-d1e1da243631",
   "metadata": {},
   "source": [
    "Ta có thể convert các vector text thành embedding vector với OpenAI như sau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fcbe3b63-9d66-4443-b1cc-4edc8caf6036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:57:10.214561Z",
     "iopub.status.busy": "2025-01-14T08:57:10.213969Z",
     "iopub.status.idle": "2025-01-14T08:57:10.218972Z",
     "shell.execute_reply": "2025-01-14T08:57:10.218386Z",
     "shell.execute_reply.started": "2025-01-14T08:57:10.214530Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4dfeeeed-9597-4ae8-930d-6946380f6643",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:57:10.220233Z",
     "iopub.status.busy": "2025-01-14T08:57:10.219956Z",
     "iopub.status.idle": "2025-01-14T08:57:10.243673Z",
     "shell.execute_reply": "2025-01-14T08:57:10.243048Z",
     "shell.execute_reply.started": "2025-01-14T08:57:10.220207Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f99816-fc55-4a63-8920-fa88bdc7f68a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Tạo đoạn text chunk đầu tiên thành vector embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8bdf47d4-2831-4fbb-a30b-94b73a9fb914",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T08:58:19.791268Z",
     "iopub.status.busy": "2025-01-14T08:58:19.790838Z",
     "iopub.status.idle": "2025-01-14T08:58:20.152390Z",
     "shell.execute_reply": "2025-01-14T08:58:20.151771Z",
     "shell.execute_reply.started": "2025-01-14T08:58:19.791238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vector: 1536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.004783688113093376,\n",
       " 0.0018869912018999457,\n",
       " -0.011884734034538269,\n",
       " 0.000676999450661242,\n",
       " -0.03792521357536316]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the first text chunk into a vector with the embedding\n",
    "query_result = embeddings.embed_query(texts[0].page_content)\n",
    "print(f\"Length of vector: {len(query_result)}\")\n",
    "query_result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258ca67a-ac83-4d72-8e66-495703eaf538",
   "metadata": {},
   "source": [
    "Ta có thể lưu trữ toàn bộ vector database với pinecone như sau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8e8f918e-7d6d-4098-acad-248261965825",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T09:10:15.573097Z",
     "iopub.status.busy": "2025-01-14T09:10:15.572456Z",
     "iopub.status.idle": "2025-01-14T09:10:17.097615Z",
     "shell.execute_reply": "2025-01-14T09:10:17.096755Z",
     "shell.execute_reply.started": "2025-01-14T09:10:15.573060Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install pinecone langchain-pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4743233b-8b40-4192-9a8d-fb33a285e56b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T09:10:17.099684Z",
     "iopub.status.busy": "2025-01-14T09:10:17.099318Z",
     "iopub.status.idle": "2025-01-14T09:10:17.107430Z",
     "shell.execute_reply": "2025-01-14T09:10:17.106805Z",
     "shell.execute_reply.started": "2025-01-14T09:10:17.099643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.control.pinecone.Pinecone at 0x7f15de5a0510>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install pinecone langchain-pinecone\n",
    "import os\n",
    "from pinecone import Pinecone as pinecone\n",
    "from langchain_pinecone  import Pinecone\n",
    "\n",
    "pinecone(\n",
    "    api_key=os.getenv('PINECONE_API_KEY'),  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eb243a81-3da4-4cfd-b7b9-d537505ad854",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T09:10:17.109316Z",
     "iopub.status.busy": "2025-01-14T09:10:17.108450Z",
     "iopub.status.idle": "2025-01-14T09:10:17.114100Z",
     "shell.execute_reply": "2025-01-14T09:10:17.113442Z",
     "shell.execute_reply.started": "2025-01-14T09:10:17.109288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x7f15d41a4c50>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7f15d416ac10>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5fe8b4e6-51d3-473c-9020-858eb5953db5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T09:10:17.116261Z",
     "iopub.status.busy": "2025-01-14T09:10:17.115976Z",
     "iopub.status.idle": "2025-01-14T09:10:18.795442Z",
     "shell.execute_reply": "2025-01-14T09:10:18.794788Z",
     "shell.execute_reply.started": "2025-01-14T09:10:17.116236Z"
    }
   },
   "outputs": [],
   "source": [
    "index_name = \"langchain-quickstart\"\n",
    "search = Pinecone.from_documents(texts, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17208463-78c3-41da-9cd9-dab6c226ee17",
   "metadata": {},
   "source": [
    "Sau khi upload lên pinecone, ta có thể tạo thấy các đoạn text chunk tương ứng với các `vector embedding` đã được upload lên pinecone và có thể thực hiện tìm kiếm các thông tin phù hợp nhất. Quá trình này còn được gọi là `Retrievers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "95dc458a-0e96-4233-9a05-7c65277a5c48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T09:10:18.796858Z",
     "iopub.status.busy": "2025-01-14T09:10:18.796402Z",
     "iopub.status.idle": "2025-01-14T09:10:19.312302Z",
     "shell.execute_reply": "2025-01-14T09:10:19.311520Z",
     "shell.execute_reply.started": "2025-01-14T09:10:18.796823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='38703253-5675-4cea-b19a-ab350195af10', metadata={}, page_content='So, the autoencoder is like your helpful magical toy box that learns to make organizing your toys'), Document(id='f077393d-ee67-486b-9d15-9aca9fd405a3', metadata={}, page_content='People use autoencoders for lots of things! Sometimes, they help computers understand images better'), Document(id='c5a64ce5-e288-4028-b7e5-7b7891ac2f4d', metadata={}, page_content='can rely on our little autoencoder friend!'), Document(id='6d2dc406-8f9e-4f6f-a89e-777445222012', metadata={}, page_content='Okay! Let’s think about a fun way to understand what an autoencoder is.')]\n"
     ]
    }
   ],
   "source": [
    "# Do a simple vector similarity search\n",
    "query = \"What is magical about an autoencoder?\"\n",
    "result = search.similarity_search(query)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7240a566-bf52-48cc-acfa-a7f451c8cd11",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0be30b-b639-4405-8704-371d471209a2",
   "metadata": {},
   "source": [
    "Một trong các hạn chế rõ rệt của LLM là khả năng thực hiện các công cụ ở ngoài LLM như Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "170a582a-6924-425d-9272-8f77bbc0d186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T09:10:19.313842Z",
     "iopub.status.busy": "2025-01-14T09:10:19.313508Z",
     "iopub.status.idle": "2025-01-14T09:10:21.314270Z",
     "shell.execute_reply": "2025-01-14T09:10:21.313138Z",
     "shell.execute_reply.started": "2025-01-14T09:10:19.313816Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain-experimental;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e3979215-ff7a-43c2-87a0-1d03e728af0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T09:10:21.316270Z",
     "iopub.status.busy": "2025-01-14T09:10:21.315747Z",
     "iopub.status.idle": "2025-01-14T09:10:21.327765Z",
     "shell.execute_reply": "2025-01-14T09:10:21.326930Z",
     "shell.execute_reply.started": "2025-01-14T09:10:21.316236Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_experimental.tools import PythonREPLTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b6ee3f08-7357-433e-b1b2-4743dfdf63bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T09:10:21.329064Z",
     "iopub.status.busy": "2025-01-14T09:10:21.328661Z",
     "iopub.status.idle": "2025-01-14T09:10:21.332162Z",
     "shell.execute_reply": "2025-01-14T09:10:21.331431Z",
     "shell.execute_reply.started": "2025-01-14T09:10:21.329040Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.tools import Tool\n",
    "from langchain_experimental.utilities import PythonREPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f1da144b-f1c9-4f50-9687-b20efcad5328",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T09:10:21.333093Z",
     "iopub.status.busy": "2025-01-14T09:10:21.332832Z",
     "iopub.status.idle": "2025-01-14T09:10:21.336629Z",
     "shell.execute_reply": "2025-01-14T09:10:21.335912Z",
     "shell.execute_reply.started": "2025-01-14T09:10:21.333067Z"
    }
   },
   "outputs": [],
   "source": [
    "python_repl = PythonREPL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3088c0c3-8c5a-484d-a1fc-4fcdb34f7e22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T09:10:21.339538Z",
     "iopub.status.busy": "2025-01-14T09:10:21.339091Z",
     "iopub.status.idle": "2025-01-14T09:10:21.354704Z",
     "shell.execute_reply": "2025-01-14T09:10:21.353741Z",
     "shell.execute_reply.started": "2025-01-14T09:10:21.339507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2\\n'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_repl.run(\"print(1+1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c9bba1cb-1f7c-418a-83e9-4acee19b1d83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T09:10:21.355620Z",
     "iopub.status.busy": "2025-01-14T09:10:21.355357Z",
     "iopub.status.idle": "2025-01-14T09:10:21.395127Z",
     "shell.execute_reply": "2025-01-14T09:10:21.394225Z",
     "shell.execute_reply.started": "2025-01-14T09:10:21.355594Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType, Tool\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=[PythonREPLTool()],\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "edc647f6-f059-4205-9d0e-bec39b8c34d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T09:10:21.396729Z",
     "iopub.status.busy": "2025-01-14T09:10:21.396156Z",
     "iopub.status.idle": "2025-01-14T09:10:22.729733Z",
     "shell.execute_reply": "2025-01-14T09:10:22.729155Z",
     "shell.execute_reply.started": "2025-01-14T09:10:21.396698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is 2 + 2?', 'output': '4'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke(\"what is 2 + 2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3a9f69-498d-4bc0-81fa-58f23cefe3d2",
   "metadata": {},
   "source": [
    "## Tài liệu tham khảo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15074cf5-0d94-48b2-b6f6-ca3d78d1dcef",
   "metadata": {},
   "source": [
    "- [Langchain in 13 minutes](https://www.youtube.com/watch?v=aywZrzNaKjs)\n",
    "- [Langchain tutorials](https://github.com/anhhd/langchain-tutorials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
