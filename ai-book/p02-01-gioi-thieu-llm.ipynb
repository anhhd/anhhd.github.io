{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e37bfa58-d454-4b37-85e9-a097921eefd7",
   "metadata": {},
   "source": [
    "# Giới thiệu về mô hình ngôn ngữ lớn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235221b0-7f82-4e10-8eaa-53f2600ba7e4",
   "metadata": {},
   "source": [
    "## Giới thiệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3480aab-def7-4e06-a3d1-28ce37637e46",
   "metadata": {},
   "source": [
    "> LLM là mạng thần kinh được thiết kế để hiểu, tạo và phản hồi văn bản giống con người. Những mô hình này là các mạng lưới thần kinh sâu được đào tạo dựa trên lượng dữ liệu văn bản khổng lồ, đôi khi bao gồm các phần lớn của toàn bộ văn bản có sẵn công khai trên internet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d664b0-5632-4137-951e-20d68dce003f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T08:52:42.027980Z",
     "iopub.status.busy": "2025-01-03T08:52:42.027670Z",
     "iopub.status.idle": "2025-01-03T08:52:42.036160Z",
     "shell.execute_reply": "2025-01-03T08:52:42.035320Z",
     "shell.execute_reply.started": "2025-01-03T08:52:42.027958Z"
    }
   },
   "source": [
    "\"Lớn\" trong \"mô hình ngôn ngữ lớn\" đề cập đến cả kích thước của mô hình về mặt tham số và tập dữ liệu khổng lồ mà nó được đào tạo. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88ae78ad-7ce7-43f7-85d6-543372f57e3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T08:53:09.543599Z",
     "iopub.status.busy": "2025-01-03T08:53:09.543299Z",
     "iopub.status.idle": "2025-01-03T08:53:10.179445Z",
     "shell.execute_reply": "2025-01-03T08:53:10.178715Z",
     "shell.execute_reply.started": "2025-01-03T08:53:09.543578Z"
    }
   },
   "source": [
    "![](image/p02-01-size-llm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a221671-ceac-4d56-9b8d-1edafc302c22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T08:53:53.804028Z",
     "iopub.status.busy": "2025-01-03T08:53:53.803680Z",
     "iopub.status.idle": "2025-01-03T08:53:53.812634Z",
     "shell.execute_reply": "2025-01-03T08:53:53.811466Z",
     "shell.execute_reply.started": "2025-01-03T08:53:53.804001Z"
    }
   },
   "source": [
    "Các mô hình LLM thường có hàng chục, thậm chí hàng trăm tỷ tham số, là các trọng số có thể điều chỉnh trong mạng thần kinh được tối ưu hóa trong quá trình đào tạo để dự đoán từ tiếp theo trong chuỗi. Dự đoán từ tiếp theo rất hợp lý vì nó khai thác tính chất tuần tự vốn có của ngôn ngữ để đào tạo các mô hình về cách hiểu ngữ cảnh, cấu trúc và các mối quan hệ trong văn bản."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deb250a-9b96-485f-9168-b44057bd267f",
   "metadata": {},
   "source": [
    "![](image/p02-01-dl-llm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d268b25c-458b-456f-9c03-5ddaddf8fe1c",
   "metadata": {},
   "source": [
    "Khác so với các mô hình NLP truyền thống vốn được huấn luyện để thực hiện 1 tác vụ cụ thể, LLM được đào tạo với `unlabel data`. Để có thể thực hiện thêm các tác vụ khác, LLM cần được `tuning` thêm.\n",
    "\n",
    "Có 2 loại tinh chỉnh (`tuning`) cơ bản của LLM là tinh chỉnh hướng dẫn (`instruction fine-tuning`) và tinh chỉnh phân loại (`classification fine-tuning`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc8a4bf-68ff-41ab-a376-e9aaa0603a8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T08:59:23.693312Z",
     "iopub.status.busy": "2025-01-03T08:59:23.693001Z",
     "iopub.status.idle": "2025-01-03T08:59:24.315130Z",
     "shell.execute_reply": "2025-01-03T08:59:24.314409Z",
     "shell.execute_reply.started": "2025-01-03T08:59:23.693289Z"
    }
   },
   "source": [
    "![](image/p02-01-train-llm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebd42fb-e941-402f-9fd1-c828fde8fd77",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56e0ee7-0bfb-4a2e-b104-fddc286db475",
   "metadata": {},
   "source": [
    "Các mô hình LLM hiện đại đều dựa vào kiến trúc của `transformer` - là một mạng thần kinh hiện đại được giới thiệu vào năm 2017 (bài báo: *transformer is all you need*). Kiến trúc này giới thiệu mô hình học máy phục vụ công tác dịch thuật."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626a6667-91e2-411b-ae75-2a687e6e44c8",
   "metadata": {},
   "source": [
    "![](image/p02-01-encode-llm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a978f1ae-e2bb-4242-a5b9-5bb6dc06455b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T09:02:09.733725Z",
     "iopub.status.busy": "2025-01-03T09:02:09.733375Z",
     "iopub.status.idle": "2025-01-03T09:02:09.740667Z",
     "shell.execute_reply": "2025-01-03T09:02:09.739487Z",
     "shell.execute_reply.started": "2025-01-03T09:02:09.733700Z"
    }
   },
   "source": [
    "Kiến trúc này gồm 2 module - bộ mã hóa (`encoder`) và giải mã (`decoder`). Module encoder sẽ mã hóa văn bản đầu vào thành vector chuỗi số hoặc vector nắm bắt thông tin ngữ cảnh đầu vào. Sau đó bộ giải mã sẽ chuyển các vector này thành ngôn ngữ đích. Hai module này liên kết với nhau bằng cơ chế `tự chú ý` (self attention).\n",
    "\n",
    "Google sau đó đã phát triển kiến trúc BERT dựa trên nền tảng Transformers nhưng tập trung sâu vào khía cạnh dự đoán các từ bị ẩn. Điều này cho phép BERT phát triển tốt các mô hình đánh giá cảm xúc. Các mô hình GPT sẽ làm tốt hơn các tác vụ như tổng hợp văn bản, viết tiểu thuyết.m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ff596b-946c-49d5-8656-69b9ad383ca8",
   "metadata": {},
   "source": [
    "![](image/p02-01-bert-gpt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4b524b-0783-4a49-b4fb-dc2c06703f29",
   "metadata": {},
   "source": [
    "## Quy trình xây dựng LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7493770-c7ef-49f2-929e-6ad36965150f",
   "metadata": {},
   "source": [
    "![](image/p02-01-train-llm.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
